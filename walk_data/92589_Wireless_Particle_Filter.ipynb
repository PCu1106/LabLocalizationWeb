{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Master_Code_92589')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_class = np.array([\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\\n",
    "                        [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,31],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,32],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,33],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,34],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,35],\\\n",
    "                        [ 0,11,12,13,14,15,16,17,18,19,20,36],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,37],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,38],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,39],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,40],\\\n",
    "                        [ 0,21,22,23,24,25,26,27,28,29,30,41],\\\n",
    "                        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "def class_to_coordinate(a):\n",
    "    table = np.array([\\\n",
    "        [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\\n",
    "        [ 0, 1, 0, 0, 0, 0,11, 0, 0, 0, 0,21],\\\n",
    "        [ 0, 2, 0, 0, 0, 0,12, 0, 0, 0, 0,22],\\\n",
    "        [ 0, 3, 0, 0, 0, 0,13, 0, 0, 0, 0,23],\\\n",
    "        [ 0, 4, 0, 0, 0, 0,14, 0, 0, 0, 0,24],\\\n",
    "        [ 0, 5, 0, 0, 0, 0,15, 0, 0, 0, 0,25],\\\n",
    "        [ 0, 6, 0, 0, 0, 0,16, 0, 0, 0, 0,26],\\\n",
    "        [ 0, 7, 0, 0, 0, 0,17, 0, 0, 0, 0,27],\\\n",
    "        [ 0, 8, 0, 0, 0, 0,18, 0, 0, 0, 0,28],\\\n",
    "        [ 0, 9, 0, 0, 0, 0,19, 0, 0, 0, 0,29],\\\n",
    "        [ 0,10, 0, 0, 0, 0,20, 0, 0, 0, 0,30],\\\n",
    "        [ 0,31,32,33,34,35,36,37,38,39,40,41]])\n",
    "    \n",
    "    x = np.argwhere(table == a)[0][1]\n",
    "    y = np.argwhere(table == a)[0][0]\n",
    "    \n",
    "    coordinate = [x,y]\n",
    "    \n",
    "    return coordinate\n",
    "\n",
    "def add_weight(particle_weight_map, pos, particle_num, alpha, c, spread = 3):\n",
    "    \n",
    "    # add weight to target position, and spread to neighbor\n",
    "    alpha_1 = alpha*c\n",
    "    alpha_2 = alpha_1*c\n",
    "    alpha_3 = alpha_2*c\n",
    "    \n",
    "    for direction in range(4):\n",
    "        particle_weight_map = add_weight_dir(particle_weight_map, pos, direction, alpha * particle_num)\n",
    "    \n",
    "    x = position_list[pos][0]\n",
    "    y = position_list[pos][1]\n",
    "    \n",
    "    if spread > 0:\n",
    "        \n",
    "        neighbor_1 = [[x+1, y], [x,y+1], [x-1,y], [x,y-1]]\n",
    "        neighbor_2 = [[x+1,y+1], [x-1,y+1], [x+1,y-1], [x-1,y-1], [x+2,y], [x,y+2], [x-2,y], [x,y-2]]\n",
    "        neighbor_3 = [[x+1,y+2], [x+2,y+1], [x+2,y-1], [x+1,y-2], [x-1,y-2], [x-2,y-1], [x-2,y+1], [x-1,y+2],\\\n",
    "                      [x+3, y], [x,y+3], [x-3,y], [x,y-3]]\n",
    "        \n",
    "        neighbor_list = [neighbor_1, neighbor_2, neighbor_3]\n",
    "        parameter_list = [alpha_1, alpha_2, alpha_3]\n",
    "                \n",
    "        for i in range(spread): \n",
    "            for neighbor_pos in neighbor_list[i]:\n",
    "                if neighbor_pos in position_list:\n",
    "                    np = position_list.index(neighbor_pos)\n",
    "                    for direction in range(4):\n",
    "                        particle_weight_map = add_weight_dir(particle_weight_map, np, direction, parameter_list[i] * particle_num)\n",
    "                    \n",
    "    return particle_weight_map\n",
    "\n",
    "def add_weight_dir(particle_weight_map, pos, direction, particle_num):\n",
    "    coor = position_list[pos]\n",
    "    particle_weight_map[coor[0]][coor[1]][direction] += particle_num\n",
    "    return particle_weight_map\n",
    "\n",
    "def proportion_init(particle_weight_map, p_count_knn):\n",
    "    \n",
    "    # spread particles\n",
    "    w_sum = 0\n",
    "    for i in range(int(p_count_knn/4)):\n",
    "        for d in range(4):\n",
    "            pos = position_list[ random.randint(0,33) ]\n",
    "            particle_weight_map[pos[0]][pos[1]][d] += (1/p_count_knn)\n",
    "            w_sum += (1/p_count_knn)\n",
    "    \n",
    "    return particle_weight_map\n",
    "\n",
    "def euclidean_distance(p1,p2):\n",
    "    return np.sqrt((p1[0]-p2[0])*(p1[0]-p2[0])+(p1[1]-p2[1])*(p1[1]-p2[1]))\n",
    "\n",
    "def avg_normalization(particle_weight_map_i):\n",
    "    w_sum = 0\n",
    "    for pos in position_list:\n",
    "        for d in range(4):\n",
    "            w_sum += particle_weight_map_i[pos[0]][pos[1]][d]\n",
    "            \n",
    "    for pos in position_list:\n",
    "        for d in range(4):\n",
    "            particle_weight_map_i[pos[0]][pos[1]][d] /= w_sum\n",
    "            \n",
    "    return particle_weight_map_i\n",
    "\n",
    "def min_max_normalization(particle_weight_map_i):\n",
    "    Min = 9999999\n",
    "    Max = 0\n",
    "    for pos in position_list:\n",
    "        for d in range(4):\n",
    "            if particle_weight_map_i[pos[0]][pos[1]][d] > Max:\n",
    "                Max = particle_weight_map_i[pos[0]][pos[1]][d]\n",
    "            if particle_weight_map[pos[0]][pos[1]][d] < Min:\n",
    "                Min = particle_weight_map_i[pos[0]][pos[1]][d]\n",
    "    for pos in position_list:\n",
    "        for d in range(4):\n",
    "            particle_weight_map_i[pos[0]][pos[1]][d] = (particle_weight_map_i[pos[0]][pos[1]][d]-Min)/Max-Min\n",
    "            \n",
    "    return particle_weight_map_i\n",
    "\n",
    "def normalization(particle_weight_map_i):\n",
    "    return avg_normalization(particle_weight_map_i)\n",
    "\n",
    "def particle_move(particle_weight_map):\n",
    "    new_map = np.zeros((13,13,4))\n",
    "    for pos in position_list: # for every particle\n",
    "        \n",
    "        if [pos[0] - 1, pos[1]] in position_list:\n",
    "            new_map[pos[0] - 1][pos[1]][0] += particle_weight_map[pos[0]][pos[1]][0]*1 # 0: x-1\n",
    "        if [pos[0], pos[1] + 1] in position_list:\n",
    "            new_map[pos[0]][pos[1] + 1][1] += particle_weight_map[pos[0]][pos[1]][1]*1 # 1: y+1\n",
    "        if [pos[0] + 1, pos[1]] in position_list:\n",
    "            new_map[pos[0] + 1][pos[1]][2] += particle_weight_map[pos[0]][pos[1]][2]*1 # 2: x+1\n",
    "        if [pos[0], pos[1] - 1] in position_list:\n",
    "            new_map[pos[0]][pos[1] - 1][3] += particle_weight_map[pos[0]][pos[1]][3]*1 # 3: y+1\n",
    "            \n",
    "    return new_map\n",
    "\n",
    "def show_map(particle_weight_map_i, flag, walk, step, true_position, true_heading, max_pos, statement, note):\n",
    "#     particle_weight_map_i = normalization(particle_weight_map_i)\n",
    "    x = particle_weight_map.shape[0]\n",
    "    y = particle_weight_map.shape[1]\n",
    "    \n",
    "    if flag:\n",
    "        show_map = np.zeros((x,y))\n",
    "        for d in range(4):\n",
    "            for ix in range(x):\n",
    "                for iy in range(y):\n",
    "                    if [ix,iy] in position_list:\n",
    "                        show_map[ix][iy] += particle_weight_map_i[ix][iy][d]\n",
    "                    else:\n",
    "                        show_map[ix][iy] = -0.05\n",
    "#         print(f'{show_map}')\n",
    "        figure = plt.figure()\n",
    "        axes = figure.add_subplot(111)\n",
    "        caxes = axes.matshow(show_map, cmap = plt.get_cmap('Blues'))\n",
    "        figure.colorbar(caxes) \n",
    "        \n",
    "        plt.title(f'Step {step}, True:{true_position[i]}, Predicted:{max_pos}')\n",
    "\n",
    "        plt.xticks(np.arange(0, 13, step=1))#设置刻度\n",
    "        plt.yticks(np.arange(0, 13, step=1))\n",
    "        \n",
    "        axes.xaxis.tick_bottom()\n",
    "        \n",
    "        if not os.path.exists(f'{walk}_map_res'):\n",
    "            os.makedirs(f'{walk}_map_res')\n",
    "            \n",
    "        if not os.path.exists(f'{walk}_map_res/{statement}') and statement != '':\n",
    "            os.makedirs(f'{walk}_map_res/{statement}')\n",
    "            \n",
    "        plt.savefig(f'{walk}_map_res/{statement}/{step}_{note}.png', dpi=200)\n",
    "        \n",
    "        plt.show()\n",
    "#         print(f'sum: {np.sum(show_map)}') \n",
    "    else:\n",
    "        show_map = np.zeros((x,y))\n",
    "        for ix in range(x):\n",
    "            for iy in range(y):\n",
    "                if [ix,iy] in position_list:\n",
    "                    show_map[ix][iy] = particle_weight_map_i[ix][iy][true_heading]\n",
    "                else:\n",
    "                    show_map[ix][iy] = -1* np.mean(particle_weight_map_i)\n",
    "#         print(f'{note}')\n",
    "#         print(f'{show_map}')\n",
    "        figure = plt.figure()\n",
    "        axes = figure.add_subplot(111)\n",
    "        caxes = axes.matshow(show_map, cmap = plt.get_cmap('Blues'))\n",
    "        figure.colorbar(caxes) \n",
    "        \n",
    "        plt.title(f'Step:{step},True:{true_position[i]},Predicted:{max_pos}')\n",
    "\n",
    "        plt.xticks(np.arange(0, 13, step=1))#\n",
    "        plt.yticks(np.arange(0, 13, step=1))\n",
    "        \n",
    "        axes.xaxis.tick_bottom()\n",
    "        \n",
    "        if not os.path.exists(f'{walk}_map_res'):\n",
    "            os.makedirs(f'{walk}_map_res')\n",
    "            \n",
    "        if not os.path.exists(f'{walk}_map_res/{statement}') and statement != '':\n",
    "            os.makedirs(f'{walk}_map_res/{statement}')\n",
    "            \n",
    "        plt.savefig(f'{walk}_map_res/{statement}/{step}_{note}.png', dpi=200)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_count_knn = 2000\n",
    "best_distance_error_list = []\n",
    "\n",
    "position_list = [[1,1],[1,2],[1,3],[1,4],[1,5],[1,6],[1,7],[1,8],[1,9],[1,10],\n",
    "                 [6,1],[6,2],[6,3],[6,4],[6,5],[6,6],[6,7],[6,8],[6,9],[6,10],\n",
    "                 [11,1],[11,2],[11,3],[11,4],[11,5],[11,6],[11,7],[11,8],[11,9],[11,10],\n",
    "                 [11,11],[10,11],[9,11],[8,11],[7,11],[6,11],[5,11],[4,11],[3,11],[2,11],[1,11]]\n",
    "# without [7,8],[7,9],[7,10],\n",
    "\n",
    "particle_weight_map = np.zeros((13,13,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "c = 0.5\n",
    "statement = ''\n",
    "\n",
    "scripted_walk = [\n",
    "    'one_way_walk_1','one_way_walk_2','one_way_walk_3','one_way_walk_4','one_way_walk_5','one_way_walk_6','one_way_walk_7','one_way_walk_8',\\\n",
    "    'round_trip_walk_2','round_trip_walk_3','round_trip_walk_4'\n",
    "]\n",
    "\n",
    "stationary = [\n",
    "    'stationary_1'\n",
    "]\n",
    "\n",
    "freewalk = [\n",
    "    'freewalk_1','freewalk_3','freewalk_4','freewalk_5','freewalk_6','freewalk_8','freewalk_9'\n",
    "]#,'freewalk_2','freewalk_7'\n",
    "\n",
    "walk_list = freewalk\n",
    "obj_missing_rate = 1 # 1 0.8 0.6\n",
    "beacon_count = 6\n",
    "phone_status = 'ideal' # ideal 2bad 2bad5\n",
    "\n",
    "\n",
    "method =  [f'1_Hashing_hTCU19e',f'1_KNN_hTCU19e',\\\n",
    "            f'1_Hashing_sharp025',f'1_KNN_sharp025',\\\n",
    "            f'1_Hashing_GalaxyA51',f'1_KNN_GalaxyA51',\\\n",
    "            f'1_Hashing_hTCU11',f'1_KNN_hTCU11',\\\n",
    "            f'2_Hashing_hTCU19e+sharp025',f'2_KNN_hTCU19e+sharp025',\\\n",
    "            f'2_Hashing_hTCU19e+GalaxyA51',f'2_KNN_hTCU19e+GalaxyA51',\\\n",
    "            f'2_Hashing_sharp025+GalaxyA51',f'2_KNN_sharp025+GalaxyA51',\\\n",
    "            f'3_Hashing_hTCU19e+sharp025+GalaxyA51',f'3_KNN_hTCU19e+sharp025+GalaxyA51',\\\n",
    "            f'4_Hashing_ALL',f'4_KNN_ALL'            \n",
    "]\n",
    "\n",
    "method =  [f'1_Hashing_hTCU19e',f'1_Hashing_sharp025',f'1_Hashing_GalaxyA51',f'1_Hashing_hTCU11',\\\n",
    "            f'2_Hashing_hTCU19e+sharp025',f'2_Hashing_hTCU19e+GalaxyA51',f'2_Hashing_sharp025+GalaxyA51',\\\n",
    "            f'3_Hashing_hTCU19e+sharp025+GalaxyA51',f'4_Hashing_ALL',\\\n",
    "            f'1_KNN_hTCU19e',f'1_KNN_sharp025',f'1_KNN_GalaxyA51',f'1_KNN_hTCU11',\\\n",
    "            f'2_KNN_hTCU19e+sharp025',f'2_KNN_hTCU19e+GalaxyA51',f'2_KNN_sharp025+GalaxyA51',\\\n",
    "            f'3_KNN_hTCU19e+sharp025+GalaxyA51',f'4_KNN_ALL'\n",
    "]\n",
    "\n",
    "methods = [f'4_Hashing_ALL']\n",
    "\n",
    "method = [f'alpha_100',f'alpha_82',f'alpha_64',f'alpha_55',f'alpha_46',f'alpha_28',f'alpha_010']\n",
    "\n",
    "\n",
    "particle_prev_count = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_distribution = {}\n",
    "\n",
    "for data_type in methods:\n",
    "    error_distribution[data_type] = []\n",
    "    \n",
    "    missing_statement = ''\n",
    "    if obj_missing_rate != 1:\n",
    "        missing_statement = f'_{int(obj_missing_rate*10)}'\n",
    "\n",
    "    Converge = {}\n",
    "    for W in walk_list:\n",
    "        Converge[W] = []\n",
    "\n",
    "    spread = 3\n",
    "\n",
    "    for walk in walk_list:\n",
    "\n",
    "        file_name = f'./92589_wireless_{data_type}_{walk}.csv'\n",
    "        \n",
    "        data = pd.read_csv(file_name, delimiter=',')\n",
    "\n",
    "        knn_candidates = []\n",
    "        true_position = []\n",
    "        true_position_class = []\n",
    "        voter_wireless_reference_point = []\n",
    "        \n",
    "        # Read knn result and ground truth from the file\n",
    "        for index, row in data.iterrows():\n",
    "            ans_list = eval(row[data_type])\n",
    "            label = class_to_coordinate(row['label'])\n",
    "            label_class = row['label']\n",
    "            knn_candidates.append(Counter(ans_list))\n",
    "            true_position.append(label)\n",
    "            true_position_class.append(label_class)\n",
    "            voter_wireless_reference_point.append(ans_list)\n",
    "\n",
    "        particle_weight_map = np.zeros((13,13,4))\n",
    "\n",
    "        predict_pos_list = []\n",
    "\n",
    "        distance_error_list = []\n",
    "\n",
    "        predicted_heading_list = [-1] # 第一個方向不知道\n",
    "\n",
    "        particle_weight_map = proportion_init(particle_weight_map, p_count_knn)\n",
    "\n",
    "        heading_list = []\n",
    "        \n",
    "        wireless_reference_point_path = f'./Result/{data_type}_{walk}_wireless_reference_point.csv'\n",
    "        \n",
    "        with open(wireless_reference_point_path, 'w', newline='') as csvfile:\n",
    "            wireless_reference_point = csv.writer(csvfile)\n",
    "            header = []\n",
    "\n",
    "            header.append('label')\n",
    "            header.append('wireless_reference_point')\n",
    "            header.append('voter_wireless_reference_point')\n",
    "            \n",
    "            wireless_reference_point.writerow(header)\n",
    "\n",
    "            for i in range(len(knn_candidates)): # for 每個 KNN 預測的位置\n",
    "\n",
    "                max_weight = 0\n",
    "                max_pos = [0, 0]\n",
    "\n",
    "                reference_point = []\n",
    "\n",
    "                if i == 0: # if 第一個位置 (是特例，沒有前一個位置)\n",
    "                    # 將該位置候選人以票數做 weight, 更新 partcle weight\n",
    "\n",
    "                    candidate_list = list(knn_candidates[i].keys()) # 每一輪預測KNN的參考位置 ( all 候選人 )\n",
    "\n",
    "                    candidate_vote_list = list(knn_candidates[i].values()) # 每一輪預測 KNN 的參考位置的次數 ( all 候選人票數 )\n",
    "\n",
    "                    all_vote = sum(candidate_vote_list) # 總票數\n",
    "\n",
    "                    # 由得票數設定 weight\n",
    "                    for k in range(len(candidate_vote_list)): # 每一個參考位置的比例分配\n",
    "                        candidate_vote_list[k] /= all_vote\n",
    "\n",
    "                    # 把 weight佈到 map上\n",
    "                    for j, element in enumerate(candidate_list): #每一輪KNN的每一個參考位置\n",
    "                        if class_to_coordinate(element) in position_list:\n",
    "                            position_list_index = position_list.index(class_to_coordinate(element))\n",
    "                            particle_weight_map = add_weight(particle_weight_map, position_list_index, 1*candidate_vote_list[j], alpha, c, spread)\n",
    "\n",
    "                    # 找地圖重心\n",
    "                    weight_sum = 0\n",
    "                    for pos in position_list:\n",
    "                        for d in range(4):\n",
    "                            if max_weight < particle_weight_map[pos[0]][pos[1]][d]:\n",
    "                                max_weight = particle_weight_map[pos[0]][pos[1]][d]\n",
    "                                max_pos = pos\n",
    "\n",
    "                    predict_pos_list.append(max_pos)\n",
    "\n",
    "                    particle_weight_map = normalization(particle_weight_map)\n",
    "\n",
    "                    C = []\n",
    "                    for pos in position_list:\n",
    "                        for d in range(4):\n",
    "                            C.append(particle_weight_map[pos[0]][pos[1]][d]*euclidean_distance(max_pos,pos)*0.6)\n",
    "                    Converge[walk].append(sum(C))\n",
    "\n",
    "                    error = round(euclidean_distance(true_position[i], predict_pos_list[-1])*0.6, 3)\n",
    "\n",
    "                    KNN_res = [(class_to_coordinate(x[0]),x[1]) for x in knn_candidates[i].most_common()]\n",
    "    #                 fp.write(f'step: {i}\\nKNN res: {KNN_res}\\nTrue: {true_position[i]}, Predicted: {max_pos}, Error: {error}\\n')\n",
    "\n",
    "                    distance_error_list.append(error)\n",
    "\n",
    "                    reference_point.append(true_position_class[i])\n",
    "                    reference_point.append(table_class[max_pos[0],max_pos[1]])\n",
    "                    reference_point.append(voter_wireless_reference_point[i])\n",
    "                    wireless_reference_point.writerow(reference_point)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    for pos in position_list:\n",
    "                        for d in range(4):\n",
    "                            particle_weight_map[pos[0]][pos[1]][d] *= particle_prev_count\n",
    "\n",
    "                    candidate_list = list(knn_candidates[i].keys()) # 每一輪預測KNN的參考位置 ( all 候選人 )\n",
    "\n",
    "                    candidate_vote_list = list(knn_candidates[i].values()) # 每一輪預測 KNN 的參考位置的次數 ( all 候選人票數 )\n",
    "\n",
    "                    all_vote = sum(candidate_vote_list) # 總票數\n",
    "\n",
    "                    # 由得票數設定 weight\n",
    "                    for k in range(len(candidate_vote_list)): # 每一個參考位置的比例分配\n",
    "                        candidate_vote_list[k] /= all_vote\n",
    "\n",
    "                    for j,element in enumerate(candidate_list): #每一輪 KNN 的每一個參考位置\n",
    "                        if class_to_coordinate(element) in position_list:\n",
    "                            position_list_index = position_list.index(class_to_coordinate(element))\n",
    "                            particle_weight_map = add_weight(particle_weight_map, position_list_index, \\\n",
    "                                                             p_count_knn*candidate_vote_list[j], alpha, c, spread)\n",
    "\n",
    "                    # 找地圖重心\n",
    "                    weight_sum = 0\n",
    "                    for pos in position_list:\n",
    "                        for d in range(4):\n",
    "                            if max_weight < particle_weight_map[pos[0]][pos[1]][d]:\n",
    "                                max_weight = particle_weight_map[pos[0]][pos[1]][d]\n",
    "                                max_pos = pos\n",
    "\n",
    "                    predict_pos_list.append(max_pos)\n",
    "\n",
    "                    particle_weight_map = normalization(particle_weight_map)\n",
    "\n",
    "                    C = []\n",
    "                    for pos in position_list:\n",
    "                        for d in range(4):\n",
    "                            C.append(particle_weight_map[pos[0]][pos[1]][d]*euclidean_distance(max_pos,pos)*0.6)\n",
    "                    Converge[walk].append(sum(C))\n",
    "\n",
    "\n",
    "                    error = round(euclidean_distance(true_position[i], predict_pos_list[-1])*0.6, 3)\n",
    "\n",
    "                    KNN_res = [(class_to_coordinate(x[0]),x[1]) for x in knn_candidates[i].most_common()]\n",
    "    #                 fp.write(f'step: {i}\\nKNN res: {KNN_res}\\nTrue: {true_position[i]}, Predicted: {max_pos}, Error: {error}\\n')\n",
    "\n",
    "                    distance_error_list.append(error)\n",
    "\n",
    "                    reference_point.append(true_position_class[i])\n",
    "                    reference_point.append(table_class[max_pos[0],max_pos[1]])\n",
    "                    reference_point.append(voter_wireless_reference_point[i])\n",
    "                    wireless_reference_point.writerow(reference_point)\n",
    "\n",
    "                error_distribution[data_type] += distance_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_Hashing_ALL,0.5421\n"
     ]
    }
   ],
   "source": [
    "file_name_mde = 'MDE.csv'\n",
    "with open(file_name_mde, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['method','MDE'])\n",
    "\n",
    "    for data_type in methods:\n",
    "        mde = sum(error_distribution[data_type])/len(error_distribution[data_type])\n",
    "        mde = round(mde, 4)\n",
    "        print(f'{data_type},{mde}') # w/i\n",
    "        writer.writerow([data_type,mde])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_Hashing_ALL, 0.5399792315680166, 0.5399792315680166, 0.5399792315680166, 0.7362409138110073, 0.7362409138110073, 0.7362409138110073, 0.8712357217030114, 0.8712357217030114, 0.8712357217030114, 0.9636552440290758, 0.9636552440290758, 0.9636552440290758, 0.9854620976116303, 0.9854620976116303, 0.9854620976116303, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n"
     ]
    }
   ],
   "source": [
    "with open('CDF.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    x = np.arange(0,8.2,0.2)\n",
    "    \n",
    "    writer.writerow(['CDF']+x.tolist())\n",
    "\n",
    "    for data_type in methods:\n",
    "\n",
    "        data = pd.DataFrame(error_distribution[data_type])\n",
    "        prob = []\n",
    "        \n",
    "        for i in x:\n",
    "            result = data.where(data<=i).count().sum()\n",
    "            prob.append(result/len(data))\n",
    "\n",
    "        print(f'{data_type}',*prob, sep=\", \")\n",
    "        writer.writerow([data_type]+prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_Hashing_ALL,0.5271097912481439\n"
     ]
    }
   ],
   "source": [
    "with open('Var.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['method','Var'])\n",
    "    \n",
    "    for data_type in methods:\n",
    "        print(f'{data_type},{np.var(error_distribution[data_type])}')\n",
    "        writer.writerow([data_type,np.var(error_distribution[data_type])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
